#!/usr/bin/env python3
"""JLPT N5/N4 ì–´íœ˜ vs ê¸°ì¡´ ëŒ€í™” ì½˜í…ì¸  ê°­ ë¶„ì„

ê¸°ì¡´ 22ê°œ ìƒí™©ì˜ ì–´íœ˜ë¥¼ JLPT N5/N4 ë¦¬ìŠ¤íŠ¸ì™€ ë¹„êµí•˜ì—¬
- ì´ë¯¸ ì»¤ë²„ëœ ë‹¨ì–´
- ëŒ€í™”ì—ì„œ ë¹ ì§„ ì¤‘ìš” ë‹¨ì–´
- ë³€í˜• ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ ê·¸ë£¹
ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import json
import glob
import os

# â”€â”€ 1. ê¸°ì¡´ ëŒ€í™” ì½˜í…ì¸ ì—ì„œ ì–´íœ˜ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_existing_vocab(output_dir):
    """scripts/output/*.jsonì—ì„œ ëª¨ë“  ì–´íœ˜ì™€ í‘œí˜„ì„ ì¶”ì¶œ"""
    existing = set()
    by_situation = {}

    for f in sorted(glob.glob(os.path.join(output_dir, '*.json'))):
        name = os.path.basename(f).replace('.json', '')
        data = json.load(open(f, encoding='utf-8'))
        words = set()

        # vocabulary ë°°ì—´ì—ì„œ
        for v in data.get('vocabulary', []):
            w = v.get('word_ja', '')
            if w:
                existing.add(w)
                words.add(w)

        # linesì˜ key_expressionsì—ì„œ
        for line in data.get('lines', []):
            for expr in line.get('key_expressions', []):
                existing.add(expr)
                words.add(expr)

            # ëŒ€ì‚¬ í…ìŠ¤íŠ¸ì—ì„œë„ ì£¼ìš” ë‹¨ì–´ í™•ì¸
            text = line.get('text_ja', '')
            # textëŠ” ë‚˜ì¤‘ì— ë§¤ì¹­ì— ì‚¬ìš©

        by_situation[name] = words

    return existing, by_situation


# â”€â”€ 2. JLPT N5 íšŒí™” í•„ìˆ˜ ì–´íœ˜ (ëŒ€í™”ì—ì„œ ìì£¼ ì“°ì´ëŠ” ê²ƒ ì¤‘ì‹¬) â”€â”€
# jlptsensei.comì—ì„œ ìˆ˜ì§‘í•œ 644ê°œ ì¤‘ íšŒí™”ì— í•µì‹¬ì ì¸ ë‹¨ì–´ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬
# (ìˆ«ì ì½ê¸°, í•œì ì½ê¸° ë“± ì‹œí—˜ ì „ìš© í•­ëª©ì€ ì œì™¸)

N5_CONVERSATION = {
    "ì¸ì‚¬Â·ê¸°ë³¸í‘œí˜„": [
        "ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã°ã‚“ã¯",
        "ã•ã‚ˆã†ãªã‚‰", "ã‚ã‚ŠãŒã¨ã†", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™", "ã™ã¿ã¾ã›ã‚“",
        "ã”ã‚ã‚“ãªã•ã„", "ã„ãŸã ãã¾ã™", "ã”ã¡ãã†ã•ã¾", "ãŠé¡˜ã„ã—ã¾ã™",
        "ã¯ã˜ã‚ã¾ã—ã¦", "ã‚ˆã‚ã—ã", "ãŠå…ƒæ°—ã§ã™ã‹", "å¤§ä¸ˆå¤«",
    ],
    "ì‡¼í•‘Â·ì£¼ë¬¸": [
        "ãã ã•ã„", "ã„ãã‚‰", "é«˜ã„", "å®‰ã„", "å††", "ãŠé‡‘",
        "è²·ã†", "å£²ã‚‹", "è²·ã„ç‰©", "åº—", "ãŠè“å­", "æœç‰©",
        "ãŠèŒ¶", "ã‚³ãƒ¼ãƒ’ãƒ¼", "ãŠå¼å½“", "ãŠé…’", "é£²ã¿ç‰©",
        "ç¾å‘³ã—ã„", "ç”˜ã„", "è¾›ã„", "ä¸å‘³ã„",
    ],
    "ì´ë™Â·êµí†µ": [
        "è¡Œã", "æ¥ã‚‹", "å¸°ã‚‹", "æ­©ã", "èµ°ã‚‹", "ä¹—ã‚‹", "é™ã‚Šã‚‹",
        "é§…", "é›»è»Š", "ãƒã‚¹", "ã‚¿ã‚¯ã‚·ãƒ¼", "é£›è¡Œæ©Ÿ", "è‡ªè»¢è»Š", "è»Š",
        "åœ°ä¸‹é‰„", "åˆ‡ç¬¦", "é“", "æ©‹", "äº¤å·®ç‚¹", "ä¿¡å·",
        "å³", "å·¦", "çœŸã£ç›´ã", "æ›²ãŒã‚‹", "è¿‘ã„", "é ã„",
        "åŒ—", "å—", "æ±", "è¥¿", "åœ°å›³",
    ],
    "ì¥ì†Œ": [
        "å­¦æ ¡", "ç—…é™¢", "éŠ€è¡Œ", "éƒµä¾¿å±€", "äº¤ç•ª", "å…¬åœ’",
        "å›³æ›¸é¤¨", "æ˜ ç”»é¤¨", "ãƒ‡ãƒ‘ãƒ¼ãƒˆ", "ã‚¹ãƒ¼ãƒ‘ãƒ¼", "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³",
        "å–«èŒ¶åº—", "ãƒ›ãƒ†ãƒ«", "ã‚¢ãƒ‘ãƒ¼ãƒˆ", "å®¶", "éƒ¨å±‹", "å°æ‰€",
        "ãŠé¢¨å‘‚", "ãƒˆã‚¤ãƒ¬", "ç„é–¢", "åº­", "ãƒ—ãƒ¼ãƒ«",
    ],
    "ì‹œê°„Â·ë‚ ì§œ": [
        "ä»Šæ—¥", "æ˜æ—¥", "æ˜¨æ—¥", "ä»Š", "æœ", "æ˜¼", "å¤œ", "æ™©",
        "åˆå‰", "åˆå¾Œ", "æ™‚é–“", "æœˆæ›œæ—¥", "ç«æ›œæ—¥", "æ°´æ›œæ—¥",
        "æœ¨æ›œæ—¥", "é‡‘æ›œæ—¥", "åœŸæ›œæ—¥", "æ—¥æ›œæ—¥",
        "æ¥é€±", "å…ˆé€±", "ä»Šé€±", "æ¥æœˆ", "å…ˆæœˆ", "ä»Šæœˆ",
        "æ˜¥", "å¤", "ç§‹", "å†¬", "ä¼‘ã¿", "å¤ä¼‘ã¿",
    ],
    "ì‚¬ëŒÂ·ê°€ì¡±": [
        "äºº", "ç”·", "å¥³", "å­ä¾›", "å‹é”", "å…ˆç”Ÿ", "å­¦ç”Ÿ",
        "ãŠçˆ¶ã•ã‚“", "ãŠæ¯ã•ã‚“", "ãŠå…„ã•ã‚“", "ãŠå§‰ã•ã‚“", "å¼Ÿ", "å¦¹",
        "çˆ¶", "æ¯", "å…„", "å§‰", "ä¸¡è¦ª", "å®¶æ—", "å¥¥ã•ã‚“",
    ],
    "ì¼ìƒë™ì‚¬": [
        "é£Ÿã¹ã‚‹", "é£²ã‚€", "è¦‹ã‚‹", "èã", "èª­ã‚€", "æ›¸ã", "è©±ã™",
        "èµ·ãã‚‹", "å¯ã‚‹", "æ´—ã†", "ä½œã‚‹", "æŒã¤", "ä½¿ã†", "å¾…ã¤",
        "ç«‹ã¤", "åº§ã‚‹", "å…¥ã‚‹", "å‡ºã‚‹", "å‡ºã‹ã‘ã‚‹", "é–‹ã‘ã‚‹", "é–‰ã‚ã‚‹",
        "ä»˜ã‘ã‚‹", "æ¶ˆã™", "éŠã¶", "æ³³ã", "æ•™ãˆã‚‹", "ç¿’ã†",
        "è¦šãˆã‚‹", "å¿˜ã‚Œã‚‹", "åˆ†ã‹ã‚‹", "çŸ¥ã‚‹", "æ€ã†", "è¨€ã†",
        "ç­”ãˆã‚‹", "å‘¼ã¶", "å€Ÿã‚Šã‚‹", "è²¸ã™", "è¿”ã™", "æ¸¡ã™",
        "å‹‰å¼·ã™ã‚‹", "ç·´ç¿’ã™ã‚‹", "æƒé™¤ã™ã‚‹", "æ´—æ¿¯ã™ã‚‹", "æ•£æ­©ã™ã‚‹",
        "è³ªå•ã™ã‚‹", "é›»è©±ã™ã‚‹", "çµå©šã™ã‚‹",
    ],
    "í˜•ìš©ì‚¬Â·ìƒíƒœ": [
        "å¤§ãã„", "å°ã•ã„", "é•·ã„", "çŸ­ã„", "æ–°ã—ã„", "å¤ã„",
        "åºƒã„", "ç‹­ã„", "æ˜ã‚‹ã„", "æš—ã„", "é‡ã„", "è»½ã„",
        "å¤šã„", "å°‘ãªã„", "æ—©ã„", "é…ã„", "é€Ÿã„", "å¤ªã„", "ç´°ã„",
        "å¿™ã—ã„", "æš‡", "æ¥½ã—ã„", "é¢ç™½ã„", "é›£ã—ã„", "æ˜“ã—ã„",
        "æš‘ã„", "å¯’ã„", "æš–ã‹ã„", "æ¶¼ã—ã„", "ç—›ã„",
        "ç¶ºéº—", "é™ã‹", "è³‘ã‚„ã‹", "ä¾¿åˆ©", "ä¸ä¾¿", "ä¸Šæ‰‹", "ä¸‹æ‰‹",
        "å¥½ã", "å«Œã„", "å¤§å¥½ã", "å…ƒæ°—", "ä¸ˆå¤«", "å¤§åˆ‡",
    ],
    "ë¬¼ê±´Â·ì˜ë¥˜": [
        "æœ¬", "æ–°è", "é›‘èªŒ", "è¾æ›¸", "ãƒãƒ¼ãƒˆ", "é‰›ç­†", "ãƒšãƒ³",
        "ã‹ã°ã‚“", "å‚˜", "æ™‚è¨ˆ", "çœ¼é¡", "ã‚«ãƒ¡ãƒ©", "é›»è©±",
        "ãƒ†ãƒ¬ãƒ“", "å†·è”µåº«", "æ´—æ¿¯æ©Ÿ", "ã‚¨ã‚¢ã‚³ãƒ³",
        "é´", "å¸½å­", "æœ", "ã‚·ãƒ£ãƒ„", "ãƒã‚¯ã‚¿ã‚¤",
        "æ¤…å­", "æœº", "ãƒ†ãƒ¼ãƒ–ãƒ«", "ãƒ™ãƒƒãƒ‰", "ãƒ‰ã‚¢", "çª“",
    ],
    "ìŒì‹": [
        "ã”é£¯", "ãƒ‘ãƒ³", "åµ", "è‚‰", "é­š", "é‡èœ", "æœç‰©",
        "ç‰›è‚‰", "è±šè‚‰", "é¶è‚‰", "ç‰›ä¹³", "ç ‚ç³–", "å¡©",
        "æœã”é£¯", "æ˜¼ã”é£¯", "æ™©ã”é£¯", "æ–™ç†",
    ],
    "ìì—°Â·ë‚ ì”¨": [
        "å¤©æ°—", "é›¨", "é›ª", "é¢¨", "ç©º", "å±±", "å·", "æµ·",
        "èŠ±", "æœ¨", "æ™´ã‚Œ", "æ›‡ã‚Š",
    ],
    "ì˜ë¬¸ì‚¬Â·ì ‘ì†": [
        "ä½•", "èª°", "ã©ã“", "ã„ã¤", "ã©ã†", "ã©ã‚Œ", "ã©ã®",
        "ã©ã¡ã‚‰", "ã„ãã¤", "ãªãœ", "ã©ã†ã—ã¦",
        "ã§ã‚‚", "ã ã‹ã‚‰", "ãã—ã¦", "ãã‚Œã‹ã‚‰", "ã¾ã ", "ã‚‚ã†",
        "ã¨ã¦ã‚‚", "ã¡ã‚‡ã£ã¨", "ãŸãã•ã‚“", "å°‘ã—", "å…¨éƒ¨",
        "ã„ã¤ã‚‚", "æ™‚ã€…", "ã‚ˆã", "å¤šåˆ†", "æœ¬å½“",
    ],
}

# â”€â”€ 3. JLPT N4 ì¶”ê°€ íšŒí™” ì–´íœ˜ â”€â”€
N4_CONVERSATION = {
    "ë¹„ì¦ˆë‹ˆìŠ¤Â·ì‚¬íšŒ": [
        "ä¼šè­°", "ä¼šç¤¾", "ç¤¾é•·", "éƒ¨é•·", "èª²é•·",
        "ã‚¢ãƒ«ãƒã‚¤ãƒˆ", "ä»•äº‹", "çµŒé¨“", "ç´¹ä»‹", "æŒ¨æ‹¶",
        "é€£çµ¡", "ç›¸è«‡", "æº–å‚™", "èª¬æ˜", "æ¡ˆå†…",
        "äºˆå®š", "äºˆç´„", "ç´„æŸ", "é–¢ä¿‚",
    ],
    "ì¼ìƒìƒí™œ_í™•ì¥": [
        "å¼•ã£è¶Šã™", "å±Šã‘ã‚‹", "å±Šã", "å±Šã‘å‡º",
        "æ‰•ã†", "è¶³ã‚Šã‚‹", "è¶³ã™", "å¢—ãˆã‚‹", "æ¸›ã‚‹",
        "æ¨ã¦ã‚‹", "æ‹¾ã†", "ç‰‡ä»˜ã‘ã‚‹", "å£Šã‚Œã‚‹", "å£Šã™",
        "é‹ã¶", "é›†ã‚ã‚‹", "é›†ã¾ã‚‹", "æ±ºã‚ã‚‹", "æ±ºã¾ã‚‹",
        "ç¶šã‘ã‚‹", "ç¶šã", "å¤‰ã‚ã‚‹", "å¤‰ãˆã‚‹",
        "è‚²ã¦ã‚‹", "ç”Ÿãã‚‹", "äº¡ããªã‚‹",
    ],
    "ê°ì •Â·ì˜ê²¬": [
        "å¬‰ã—ã„", "æ‚²ã—ã„", "å¯‚ã—ã„", "æ¥ãšã‹ã—ã„",
        "æ€–ã„", "é…·ã„", "å„ªã—ã„", "å³ã—ã„", "çã—ã„",
        "ç´ æ™´ã‚‰ã—ã„", "ç¾ã—ã„", "æ­£ã—ã„",
        "å®‰å¿ƒ", "å®‰å…¨", "å±é™º", "ç°¡å˜", "è¤‡é›‘",
        "ç‰¹åˆ¥", "æ™®é€š", "è‡ªç”±", "å¿…è¦",
        "æ„è¦‹", "æ°—æŒã¡", "èˆˆå‘³", "è¶£å‘³",
    ],
    "ì˜ë£ŒÂ·ê±´ê°•": [
        "æ­¯åŒ»è€…", "æ€ªæˆ‘", "ç†±", "è¡€", "æ³¨å°„",
        "å…¥é™¢", "é€€é™¢", "ãŠè¦‹èˆã„",
    ],
    "ì£¼ê±°Â·ìƒí™œ": [
        "å®¶è³ƒ", "å¸ƒå›£", "ç•³", "å£", "éšæ®µ",
        "ã‚¬ã‚¹", "æ°´é“", "æš–æˆ¿", "å†·æˆ¿",
        "ã‚´ãƒŸ", "æ´—æ¿¯ç‰©",
    ],
    "ì´ë™_í™•ì¥": [
        "ç©ºæ¸¯", "é£›è¡Œå ´", "æ¸¯", "å‚", "å³¶",
        "æ€¥è¡Œ", "ç‰¹æ€¥", "ä¹—ã‚Šæ›ãˆ",
        "é€šã†", "é€šã‚‹", "é€²ã‚€", "æˆ»ã‚‹",
    ],
    "ê²½ì–´Â·ì¡´ê²½": [
        "ã„ã‚‰ã£ã—ã‚ƒã‚‹", "å¬ã—ä¸ŠãŒã‚‹", "ã”è¦§ã«ãªã‚‹",
        "ãŠã£ã—ã‚ƒã‚‹", "ãã ã•ã‚‹", "å·®ã—ä¸ŠãŒã‚‹",
        "å‚ã‚‹", "è‡´ã™", "ä¼ºã†", "æ‹è¦‹ã™ã‚‹",
        "ã”ä¸»äºº", "ã”å­˜ã˜",
    ],
}


def flatten_dict(d):
    """ì¹´í…Œê³ ë¦¬ë³„ ë”•ì…”ë„ˆë¦¬ë¥¼ flat setìœ¼ë¡œ"""
    result = set()
    for words in d.values():
        result.update(words)
    return result


def analyze_gap(existing, jlpt_words, label):
    """ê¸°ì¡´ ì–´íœ˜ì™€ JLPT ë¦¬ìŠ¤íŠ¸ ë¹„êµ"""
    covered = existing & jlpt_words
    missing = jlpt_words - existing
    return {
        "label": label,
        "total_jlpt": len(jlpt_words),
        "covered": len(covered),
        "missing": len(missing),
        "coverage_pct": round(len(covered) / len(jlpt_words) * 100, 1),
        "covered_words": sorted(covered),
        "missing_words": sorted(missing),
    }


def categorize_missing(missing_words, category_dict):
    """ë¹ ì§„ ë‹¨ì–´ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    result = {}
    categorized = set()
    for cat, words in category_dict.items():
        cat_missing = [w for w in words if w in missing_words]
        if cat_missing:
            result[cat] = cat_missing
            categorized.update(cat_missing)
    uncategorized = sorted(set(missing_words) - categorized)
    if uncategorized:
        result["ê¸°íƒ€"] = uncategorized
    return result


def main():
    base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_dir = os.path.join(base, 'scripts', 'output')

    # 1. ê¸°ì¡´ ì–´íœ˜ ì¶”ì¶œ
    existing, by_situation = extract_existing_vocab(output_dir)
    print(f"ê¸°ì¡´ ëŒ€í™” ì½˜í…ì¸ : {len(existing)}ê°œ ê³ ìœ  ì–´íœ˜, {len(by_situation)}ê°œ ìƒí™©")

    # 2. JLPT ì–´íœ˜
    n5_words = flatten_dict(N5_CONVERSATION)
    n4_words = flatten_dict(N4_CONVERSATION)
    all_jlpt = n5_words | n4_words

    print(f"JLPT N5 íšŒí™” ì–´íœ˜: {len(n5_words)}ê°œ")
    print(f"JLPT N4 íšŒí™” ì–´íœ˜: {len(n4_words)}ê°œ")
    print(f"í•©ê³„: {len(all_jlpt)}ê°œ")
    print()

    # 3. ê°­ ë¶„ì„
    n5_gap = analyze_gap(existing, n5_words, "N5")
    n4_gap = analyze_gap(existing, n4_words, "N4")

    print(f"=== N5 ì»¤ë²„ë¦¬ì§€ ===")
    print(f"ì»¤ë²„: {n5_gap['covered']}/{n5_gap['total_jlpt']} ({n5_gap['coverage_pct']}%)")
    print(f"ë¹ ì§„ ë‹¨ì–´: {n5_gap['missing']}ê°œ")
    print()

    print(f"=== N4 ì»¤ë²„ë¦¬ì§€ ===")
    print(f"ì»¤ë²„: {n4_gap['covered']}/{n4_gap['total_jlpt']} ({n4_gap['coverage_pct']}%)")
    print(f"ë¹ ì§„ ë‹¨ì–´: {n4_gap['missing']}ê°œ")
    print()

    # 4. ë¹ ì§„ ë‹¨ì–´ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ
    n5_missing_cats = categorize_missing(set(n5_gap['missing_words']), N5_CONVERSATION)
    n4_missing_cats = categorize_missing(set(n4_gap['missing_words']), N4_CONVERSATION)

    print("=== N5 ë¹ ì§„ ë‹¨ì–´ (ì¹´í…Œê³ ë¦¬ë³„) ===")
    for cat, words in n5_missing_cats.items():
        print(f"  {cat}: {', '.join(words[:10])}" + (f" (+{len(words)-10}ê°œ)" if len(words) > 10 else ""))
    print()

    print("=== N4 ë¹ ì§„ ë‹¨ì–´ (ì¹´í…Œê³ ë¦¬ë³„) ===")
    for cat, words in n4_missing_cats.items():
        print(f"  {cat}: {', '.join(words[:10])}" + (f" (+{len(words)-10}ê°œ)" if len(words) > 10 else ""))

    # 5. ë³€í˜• ì‹œë‚˜ë¦¬ì˜¤ ì œì•ˆ
    print()
    print("=== ë³€í˜• ì‹œë‚˜ë¦¬ì˜¤ ì œì•ˆ ===")
    suggestions = suggest_variations(n5_missing_cats, n4_missing_cats, by_situation)
    for s in suggestions:
        print(f"\nğŸ“ {s['name']}")
        print(f"   ê¸°ë°˜: {s['base_situation']}")
        print(f"   ì¶”ê°€ ì–´íœ˜: {', '.join(s['new_words'][:8])}")
        print(f"   ì´ìœ : {s['reason']}")

    # 6. ê²°ê³¼ ì €ì¥
    result = {
        "summary": {
            "existing_vocab": len(existing),
            "situations": len(by_situation),
            "n5_total": len(n5_words),
            "n5_covered": n5_gap['covered'],
            "n5_coverage_pct": n5_gap['coverage_pct'],
            "n4_total": len(n4_words),
            "n4_covered": n4_gap['covered'],
            "n4_coverage_pct": n4_gap['coverage_pct'],
        },
        "n5_missing_by_category": n5_missing_cats,
        "n4_missing_by_category": n4_missing_cats,
        "variation_suggestions": suggestions,
    }

    out_path = os.path.join(base, 'data', 'gap_analysis.json')
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nê²°ê³¼ ì €ì¥: {out_path}")


def suggest_variations(n5_missing, n4_missing, by_situation):
    """ë¹ ì§„ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³€í˜• ì‹œë‚˜ë¦¬ì˜¤ ì œì•ˆ"""
    suggestions = []

    # 1. í¸ì˜ì  v2: ìŒì‹ ê´€ë ¨ + ë‚ ì”¨ ì†Œì¬
    convenience_missing = []
    for cat in ["ìŒì‹", "ì‡¼í•‘Â·ì£¼ë¬¸"]:
        convenience_missing.extend(n5_missing.get(cat, []))
    if convenience_missing:
        suggestions.append({
            "name": "convenience_store_v2 â€” ë„ì‹œë½Â·ìŒì‹ ê³ ë¥´ê¸°",
            "base_situation": "convenience_store",
            "new_words": convenience_missing[:10],
            "reason": "ê¸°ì¡´ì€ ë‹¨ìˆœ êµ¬ë§¤. ìŒì‹ ì´ë¦„(åµ, è‚‰, é‡èœ)ê³¼ ë§› í‘œí˜„(ç”˜ã„, è¾›ã„) ì¶”ê°€",
            "jlpt": "N5",
        })

    # 2. ì¹´í˜ v2: ë‚ ì”¨ ì´ì•¼ê¸° + í˜•ìš©ì‚¬
    cafe_missing = []
    for cat in ["ìì—°Â·ë‚ ì”¨", "í˜•ìš©ì‚¬Â·ìƒíƒœ"]:
        cafe_missing.extend(n5_missing.get(cat, []))
    if cafe_missing:
        suggestions.append({
            "name": "cafe_v2 â€” ë‚ ì”¨ ì´ì•¼ê¸°í•˜ë©´ì„œ ì£¼ë¬¸",
            "base_situation": "cafe",
            "new_words": cafe_missing[:10],
            "reason": "ì£¼ë¬¸ ì „ ìŠ¤ëª°í† í¬: ë‚ ì”¨(æš‘ã„, å¯’ã„, æ¶¼ã—ã„)ì™€ ê°ìƒ(ç¶ºéº—, é™ã‹)",
            "jlpt": "N5",
        })

    # 3. ê¸¸ ë¬»ê¸° v2: ì¥ì†Œ ê´€ë ¨ ì–´íœ˜ í™•ì¥
    directions_missing = []
    for cat in ["ì´ë™Â·êµí†µ", "ì¥ì†Œ"]:
        directions_missing.extend(n5_missing.get(cat, []))
    if directions_missing:
        suggestions.append({
            "name": "ask_directions_v2 â€” ì „ì²  í™˜ìŠ¹ ë¬¼ì–´ë³´ê¸°",
            "base_situation": "ask_directions",
            "new_words": directions_missing[:10],
            "reason": "ê¸°ì¡´ì€ ë‹¨ìˆœ ë°©í–¥. ì „ì² ì—­, ì§€í•˜ì² , í™˜ìŠ¹ ê´€ë ¨ ì–´íœ˜ ì¶”ê°€",
            "jlpt": "N5",
        })

    # 4. ì‹ë‹¹ v2: ê°€ì¡±Â·ì‚¬ëŒ + ìŒì‹ í™•ì¥
    restaurant_missing = []
    for cat in ["ì‚¬ëŒÂ·ê°€ì¡±", "ìŒì‹"]:
        restaurant_missing.extend(n5_missing.get(cat, []))
    if restaurant_missing:
        suggestions.append({
            "name": "restaurant_v2 â€” ê°€ì¡±ê³¼ í•¨ê»˜ ì‹ì‚¬ ì£¼ë¬¸",
            "base_situation": "restaurant",
            "new_words": restaurant_missing[:10],
            "reason": "ê°€ì¡±(ãŠçˆ¶ã•ã‚“, å­ä¾›)ê³¼ í•¨ê»˜ ë¨¹ì„ ê²ƒ ê³ ë¥´ë©° ì‚¬ëŒÂ·ìŒì‹ ì–´íœ˜ í†µí•©",
            "jlpt": "N5",
        })

    # 5. ë³‘ì› v2 (N4): ì˜ë£Œ ì–´íœ˜ í™•ì¥
    hospital_missing = n4_missing.get("ì˜ë£ŒÂ·ê±´ê°•", [])
    if hospital_missing:
        suggestions.append({
            "name": "hospital_v2 â€” ì¹˜ê³¼/ë¶€ìƒ ì„¤ëª…",
            "base_situation": "hospital",
            "new_words": hospital_missing,
            "reason": "ê¸°ì¡´ì€ ì¼ë°˜ ì§„ë£Œ. ì¹˜ê³¼(æ­¯åŒ»è€…), ë¶€ìƒ(æ€ªæˆ‘), ì£¼ì‚¬(æ³¨å°„) ë“± N4 ì˜ë£Œ ì–´íœ˜",
            "jlpt": "N4",
        })

    # 6. ë¶€ë™ì‚° v2 (N4): ì£¼ê±° ì–´íœ˜ í™•ì¥
    housing_missing = n4_missing.get("ì£¼ê±°Â·ìƒí™œ", [])
    if housing_missing:
        suggestions.append({
            "name": "real_estate_v2 â€” ì…ì£¼ í›„ ìƒí™œ ì„¤ëª…",
            "base_situation": "real_estate",
            "new_words": housing_missing,
            "reason": "ê¸°ì¡´ì€ ì§‘ êµ¬ê²½. ì…ì£¼ í›„ ê°€ìŠ¤(ã‚¬ã‚¹), ìˆ˜ë„(æ°´é“), ì“°ë ˆê¸°(ã‚´ãƒŸ) ë“± ìƒí™œ ì–´íœ˜",
            "jlpt": "N4",
        })

    # 7. ë¹„ì¦ˆë‹ˆìŠ¤ íšŒì˜ v2 (N4): ê²½ì–´ + ë¹„ì¦ˆë‹ˆìŠ¤ í™•ì¥
    biz_missing = []
    for cat in ["ê²½ì–´Â·ì¡´ê²½", "ë¹„ì¦ˆë‹ˆìŠ¤Â·ì‚¬íšŒ"]:
        biz_missing.extend(n4_missing.get(cat, []))
    if biz_missing:
        suggestions.append({
            "name": "meeting_response_v2 â€” í”„ë ˆì   í›„ ì§ˆì˜ì‘ë‹µ",
            "base_situation": "meeting_response",
            "new_words": biz_missing[:10],
            "reason": "ê²½ì–´(ã„ã‚‰ã£ã—ã‚ƒã‚‹, å¬ã—ä¸ŠãŒã‚‹)ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ì–´íœ˜(ä¼šè­°, èª¬æ˜, ç›¸è«‡) í™œìš©",
            "jlpt": "N4",
        })

    return suggestions


if __name__ == "__main__":
    main()
