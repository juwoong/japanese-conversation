# 온보딩 리디자인: 공항 도착 체험

## 목표

현재 온보딩(성별 + 페르소나 선택 폼)을 "도쿄 공항 도착" 체험형 온보딩으로 전면 교체합니다.
사용자가 첫 실행 후 60초 안에 일본어 소리를 듣고 의미를 파악하는 경험을 만듭니다.

이 플랜에는 다음 기능이 포함되어 있습니다.

1. 공항 도착 일러스트 + 직원 음성 인사 체험
2. 마이크 입력(선택) → 발음 수준 암묵 측정
3. "듣기만 할게요" 안전 경로 (강제하면 이탈)
4. 매 세션 시작 시 "지금 말할 수 있는 환경?" 분기

## 요구 결과물

1. 리디자인된 `OnboardingScreen.tsx` — 3단계 몰입형 플로우
2. 새 컴포넌트 `SessionModeSelector.tsx` — 매 세션 음성/묵음 분기
3. 시뮬레이터에서 전체 플로우 실행 확인 (크래시 없음)
4. 기존 페르소나 선택 로직이 온보딩 체험 뒤로 자연스럽게 통합되었음을 확인

## 타겟 프로젝트

`app/` 밑에 있는 메인 서비스를 대상으로 합니다.

## 전체 프로세스 주의사항

1. 하나의 스텝이 완료되면 git commit이 발생되어야 합니다.
2. 하나의 스텝이 완료되면 자동으로 다음 스텝으로 넘어가세요.
3. 모든 프로젝트 작성과, 실행을 통한 검증이 완료되기 전까지는 스텝과 루프를 종료하지 마세요.
4. 더 이상 진행할 수 없을 정도로 모호한 경우에만 중단하고 질문을 요청하세요.
5. 적절한 agent team 멤버를 구성하세요. 모든 작업에는 무조건 리뷰어가 할당되어야 하며, 각 스텝의 완료 직전에도 리뷰가 진행되어야 합니다.
6. 절대 전체 프로세스를 컨텍스트에서 관리하지 마세요. PLAN.md 파일을 이용하십시오.

## 상세 프로세스

### 1. 현재 온보딩 분석 및 리디자인 설계

현재 `OnboardingScreen.tsx`를 읽고 기존 로직(성별 선택, 페르소나 선택, Supabase 프로필 저장)을 파악한 뒤,
v1 기획서의 온보딩 플로우와 매핑합니다.

**v1 온보딩 3단계:**

```
Step 1: 공항 도착 일러스트 → "도쿄에 도착했습니다." → [시작하기]
Step 2: 공항 안내 데스크 → 직원 음성 🔊 + "따라 말해보세요"
        → [🎤 녹음] 또는 [듣기만 할게요]
        → 음성 입력 시 → 발음 수준 측정 (내부)
        → 스킵 시 → 보수적 초급 설정
Step 3: 직원이 출구 가리키기 → "첫 번째 일본어를 들었습니다!"
        → [도쿄 탐험 시작하기]
```

**설계 포인트 (반드시 반영):**
- 온보딩에 일본어 자막 없음. 소리 + 그림 + 한국어 안내만
- "듣기만 할게요" 옵션 필수
- "레벨 테스트"라는 단어 없음
- 기존 페르소나 선택은 온보딩 체험 이후, 지도 화면 첫 진입 시 자연스럽게 유도

해당 작업에는 다음 에이전틱 팀이 구성되어야 합니다.

1. UX 디자이너 — 몰입형 플로우 설계, 이탈 지점 최소화
2. 프론트엔드 엔지니어 — React Native 구현 가능성 검증

### 2. 온보딩 화면 구현

1-a에서 확정된 설계를 기반으로 `OnboardingScreen.tsx`를 리디자인합니다.

**구현 세부사항:**
- 3단계를 하나의 화면 내 state machine으로 관리 (step: 1 | 2 | 3)
- Step 2에서 음성 녹음은 기존 `audio.ts`의 `startRecording()`/`stopRecording()` 재활용
- Step 2에서 녹음된 음성은 Whisper API로 전송 → 결과의 정확도로 발음 수준 내부 판단
  - 정확도 80%+ → 중급 설정
  - 정확도 40~79% → 초급 설정
  - 스킵 또는 40% 미만 → 보수적 초급 설정
- 일러스트는 MVP에서 이모지 + 배경색으로 대체 (공항: ✈ + 하늘색 배경)
- TTS로 직원 인사 재생: `いらっしゃいませ` (expo-speech 또는 기존 TTS 설정 활용)

해당 작업에는 다음 에이전틱 팀이 구성되어야 합니다.

1. 프론트엔드 엔지니어
2. QA 엔지니어 — 3단계 각각의 경로(녹음/스킵) 테스트

### 3. 매 세션 음성/묵음 분기 구현

매 세션 시작 시 "지금 말할 수 있는 환경인가요?" 모달을 구현합니다.

```
[🎤 네, 말할 수 있어요]  → 음성 입력 모드
[🔇 지금은 조용히 할게요] → 탭 기반 모드
```

- `SessionModeSelector.tsx` 컴포넌트 생성
- 선택 결과를 `SessionScreen`에 전달 (mode: 'voice' | 'silent')
- silent 모드에서는 녹음 버튼 대신 선택지/텍스트 입력으로 대체

해당 작업에는 다음 에이전틱 팀이 구성되어야 합니다.

1. 프론트엔드 엔지니어
2. 접근성 전문가 — 묵음 모드에서의 학습 품질 보장

### 4. 통합 테스트 및 검증

시뮬레이터에서 다음 시나리오를 모두 실행합니다.

- 신규 유저: 온보딩 Step 1→2(녹음)→3 → 메인 화면
- 신규 유저: 온보딩 Step 1→2(스킵)→3 → 메인 화면
- 기존 유저: 앱 재실행 → 온보딩 스킵 → 메인 화면
- 세션 시작: 음성 모드 선택 → 세션 진행
- 세션 시작: 묵음 모드 선택 → 세션 진행

해당 작업에는 다음 에이전틱 팀이 구성되어야 합니다.

1. QA 엔지니어 2명 — 각 시나리오별 독립 테스트
2. 프론트엔드 엔지니어 — 버그 수정

## 참고사항

1. 기존 `OnboardingScreen.tsx`의 페르소나 선택 로직은 삭제하지 말고 별도 화면으로 분리합니다.
2. 일러스트 에셋은 MVP 단계에서 이모지 + 컬러 배경으로 대체합니다.
3. TTS 속도 설정은 `SettingsScreen`에서 이미 구현되어 있으므로 그대로 활용합니다.
4. `app/.env`에 필요한 API 키들이 이미 설정되어 있습니다. 절대 `.env`를 직접 읽지 마십시오.
5. 관련 v1 기획서: `docs/v1/Part1_원칙_온보딩_메인.md` 온보딩 섹션
